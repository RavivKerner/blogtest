{
  
    
        "post0": {
            "title": "Title",
            "content": "!pip install -Uqq fastbook . |████████████████████████████████| 720 kB 6.8 MB/s |████████████████████████████████| 46 kB 4.5 MB/s |████████████████████████████████| 1.2 MB 44.2 MB/s |████████████████████████████████| 189 kB 46.3 MB/s |████████████████████████████████| 56 kB 5.6 MB/s |████████████████████████████████| 51 kB 392 kB/s . import fastbook . fastbook.setup_book() . from fastbook import * . Key Loss func &amp; matrics by fastai . #MSE for loss and matric . . . def Binary_cls_loss(preds,y): preds = sigmoid(preds) return torch.where(y==1, y-preds, preds).mean() def Binary_metric_accuracy(preds, y, threshhold=0.5): pred_accuracy = (sigmoid(preds)&gt;threshhold) return (pred_accuracy==y).float().mean() . def mnist_loss(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1-predictions, predictions).mean() def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() . preds = tensor([ 9.4077, -1.3319, 9.3460, 5.9358]) y = tensor([1, 1, 0, 0]) . Binary_cls_loss(preds, y), mnist_loss(preds, y) . (tensor(0.6971), tensor(0.6971)) . preds = tensor([[ 9.4077, -1.3319], [9.3460, 5.9358],[9.3460, 5.9358], [9.3460, 5.9358] ]) . F.cross_entropy(preds,y) #will not work becuase in class he build calssifiers give only onr resoult . tensor(3.5618) . Binary_metric_accuracy(preds,y), batch_accuracy(preds,y) . (tensor(0.2500), tensor(0.2500)) . . . def softmaxp1(preds): sume = np.exp(preds).sum(1) # sume.reshape((len(preds),1)) return np.exp(preds)/sume.reshape((len(preds),1)) def multi_c_loss(preds, y): soft = softmaxp1(preds) softcon = np.log(soft)*-1 idx = range(len(y)) return softcon[idx, y].mean() def multi_accuracy(preds, y): to_C = preds.argmax(1) return (to_C==y).float().mean() . def softmax(x): return np.exp(x) / np.exp(x).sum(dim=1, keepdim=True) . def softmax(x): return np.exp(x) / np.exp(x).sum(dim=1, keepdim=True) F.nll_loss(sm_acts, targ, reduction=&#39;none&#39;) #negative log likelihood (but it asumed you already took the log...) #(its because pytorch desgin log_softmax function which take soft_max and log together) #the &#39;reduction = None &#39; io show per row and not average, by default all loss function in Pytorch takes the mean F.cross_entropy(acts, targ) #softmax+negative log likelihood # which, in practice, actually does log_softmax and then nll_loss . y = tensor([0, 1, 1, 2, 3]) . preds =tensor([[8.8227, 9.1500, 3.8286, 9.5931], [3.9045, 6.0090, 2.5657, 7.9364], [9.4077, 1.3319, 9.3460, 5.9358], [8.6940, 5.6772, 7.4109, 4.2940], [8.8544, 5.7390, 2.6658, 6.2745]]) . F.nll_loss(preds, y, reduction=&#39;none&#39;) #negative log likelihood (but it asumed you already took the log...) #(its because pytorch degin log_softmax function which take soft_max and log together) . tensor([-8.8227, -6.0090, -1.3319, -7.4109, -6.2745]) . loss_func_nn = nn.CrossEntropyLoss(reduction=&#39;none&#39;) . multi_c_loss(multipredsf, y), F.cross_entropy(multipredsf,y), loss_func_nn(multipredsf,y) #dont work directly like F . (tensor(3.3246), tensor(3.3246), tensor([1.5161, 2.0826, 8.7546, 1.5746, 2.6952])) . softmax(preds) == softmaxp1(preds) . tensor([[True, True, True, True], [True, True, True, True], [True, True, True, True], [True, True, True, True], [True, True, True, True]]) . #name of all functionsV #didnt show accuracy function . . def multi_l_loss(preds, y): sig = sigmoid(preds) dist = (sig-y).abs() return -np.log(1-dist).mean() def multi_l_accuracy(preds, y, th=0.5): sig_preds = sigmoid(preds) bin_conv = (sig_preds&gt;th).float() return (bin_conv==y).float().mean() . # F.binary_cross_entropy and its module equivalent nn.BCELoss calculate cross-entropy on a one-hot-encoded target, but do not include the initial sigmoid # F.binary_cross_entropy_with_logits (or nn.BCEWithLogitsLoss) include sigmoid # The equivalent for single-label datasets (like MNIST or the Pet dataset), where the target is encoded as a single integer, # is F.nll_loss or nn.NLLLoss for the version without the initial softmax, # and F.cross_entropy or nn.CrossEntropyLoss for the version with the initial softmax. def accuracy_multi(inp, targ, thresh=0.5, sigmoid=True): &quot;Compute accuracy when `inp` and `targ` are the same size.&quot; inp = sigmoid(inp) targ = targ.bool() return ((inp&gt;thresh)==targ).float().mean() . preds =tensor([[8.8227, 9.1500, 3.8286, 9.5931], [3.9045, 6.0090, 2.5657, 7.9364], [9.4077, 1.3319, 9.3460, 5.9358], [8.6940, 5.6772, 7.4109, 4.2940], [8.8544, 5.7390, 2.6658, 6.2745]]) y =tensor([[1, 0, 0, 0], [1, 1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 1, 1, 1]]) . y.dtype #standart pytorch wont work, need to change to float . torch.int64 . preds . tensor([[8.8227, 9.1500, 3.8286, 9.5931], [3.9045, 6.0090, 2.5657, 7.9364], [9.4077, 1.3319, 9.3460, 5.9358], [8.6940, 5.6772, 7.4109, 4.2940], [8.8544, 5.7390, 2.6658, 6.2745]]) . lss = nn.BCEWithLogitsLoss() . multi_l_loss(preds,y), lss(preds,y.float()) . (tensor(4.5089), tensor(4.5089)) . accuracy_multi(preds, y), multi_l_accuracy(preds, y) . (TensorBase(0.3500), tensor(0.3500)) . F.binary_cross_entropy_with_logits(preds,y) . RuntimeError Traceback (most recent call last) &lt;ipython-input-42-f1b49aa95af6&gt; in &lt;module&gt;() -&gt; 1 F.binary_cross_entropy_with_logits(preds,y) /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in binary_cross_entropy_with_logits(input, target, weight, size_average, reduce, reduction, pos_weight) 2980 raise ValueError(&#34;Target size ({}) must be the same as input size ({})&#34;.format(target.size(), input.size())) 2981 -&gt; 2982 return torch.binary_cross_entropy_with_logits(input, target, weight, pos_weight, reduction_enum) 2983 2984 RuntimeError: result type Float can&#39;t be cast to the desired output type Long . pdb.set_trace() multi_l_accuracy(multipredsf,y) . from fastai.vision.all import * . import pdb . accuracy_multi(multipredsf,y) . TensorBase(0.3500) . accuracy_multi?? . flatten_check?? . ((inp&gt;0.5)==y.bool()).float().mean() . tensor(0.3500) . inp=sigmoid(multipredsf) inp . tensor([[0.9999, 0.9999, 0.9787, 0.9999], [0.9802, 0.9975, 0.9286, 0.9996], [0.9999, 0.7912, 0.9999, 0.9974], [0.9998, 0.9966, 0.9994, 0.9865], [0.9999, 0.9968, 0.9350, 0.9981]]) . y.bool() . tensor([[ True, False, False, False], [ True, True, False, False], [False, False, False, False], [False, False, False, True], [False, True, True, True]]) . def Binary_cls_loss(preds,y): preds = sigmoid(preds) return torch.where(y==1, y-preds, preds).mean() . Binary_cls_loss(randres, y) . tensor(0.6971) . (sigmoid(randres)-y).abs().mean() . tensor(0.6971) . plot_function(sigmoid) . /usr/local/lib/python3.7/dist-packages/fastbook/__init__.py:74: UserWarning: Not providing a value for linspace&#39;s steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at /pytorch/aten/src/ATen/native/RangeFactories.cpp:25.) x = torch.linspace(min,max) . plot_function(torch.log, min=0,max=4) . /usr/local/lib/python3.7/dist-packages/fastbook/__init__.py:74: UserWarning: Not providing a value for linspace&#39;s steps is deprecated and will throw a runtime error in a future release. This warning will appear only once per process. (Triggered internally at ../aten/src/ATen/native/RangeFactories.cpp:23.) x = torch.linspace(min,max) . def Binary_metric_accuracy(preds, y, threshhold=0.5): pred_accuracy = sigmoid(preds)&gt;threshhold return (pred_accuracy==y).float().mean() . ((sigmoid(preds)&gt;0.5).float()==y).float().mean() . tensor(0.2500) . Binary_metric_accuracy(preds, y) . tensor(0.2500) . . . y = tensor([1, 2, 2, 3]) y . tensor([1, 2, 2, 3]) . preds . tensor([ 9.4077, -1.3319, 9.3460, 5.9358]) . preds2 = tensor([ 9.4077, -1.3319, 2.3460, 3.9358]) . def softmaxp1(preds): sume = np.exp(preds).sum(1) # sume.reshape((len(preds),1)) return np.exp(preds)/sume.reshape((len(preds),1)) . multiperds = torch.rand((5,4))*10 . multiperds . tensor([[8.8227, 9.1500, 3.8286, 9.5931], [3.9045, 6.0090, 2.5657, 7.9364], [9.4077, 1.3319, 9.3460, 5.9358], [8.6940, 5.6772, 7.4109, 4.2940], [8.8544, 5.7390, 2.6658, 6.2745]]) . multipredsf =tensor([[8.8227, 9.1500, 3.8286, 9.5931], [3.9045, 6.0090, 2.5657, 7.9364], [9.4077, 1.3319, 9.3460, 5.9358], [8.6940, 5.6772, 7.4109, 4.2940], [8.8544, 5.7390, 2.6658, 6.2745]]) . multipredsf . tensor([[8.8227, 9.1500, 3.8286, 9.5931], [3.9045, 6.0090, 2.5657, 7.9364], [9.4077, 1.3319, 9.3460, 5.9358], [8.6940, 5.6772, 7.4109, 4.2940], [8.8544, 5.7390, 2.6658, 6.2745]]) . sf = softmaxp1(multipredsf) sf . tensor([[2.1956e-01, 3.0457e-01, 1.4881e-03, 4.7438e-01], [1.5190e-02, 1.2460e-01, 3.9822e-03, 8.5622e-01], [5.0722e-01, 1.5773e-04, 4.7687e-01, 1.5753e-02], [7.4715e-01, 3.6579e-02, 2.0709e-01, 9.1731e-03], [8.9111e-01, 3.9530e-02, 1.8292e-03, 6.7530e-02]]) . logsf = np.log(sf)*-1 logsf . tensor([[1.5161, 1.1888, 6.5102, 0.7457], [4.1871, 2.0826, 5.5259, 0.1552], [0.6788, 8.7546, 0.7405, 4.1507], [0.2915, 3.3083, 1.5746, 4.6915], [0.1153, 3.2307, 6.3039, 2.6952]]) . np.exp(-1.5161) . 0.21956652877963362 . y = tensor([0, 1, 1, 2, 3]) . y . tensor([0, 1, 1, 2, 3]) . idx = range(len(y)) . logsf[idx,y].mean() . tensor(3.3246) . multipredsf[idx, y].mean(), multipredsf[idx, y] . (tensor(5.9698), tensor([8.8227, 6.0090, 1.3319, 7.4109, 6.2745])) . multipredsf . tensor([[8.8227, 9.1500, 3.8286, 9.5931], [3.9045, 6.0090, 2.5657, 7.9364], [9.4077, 1.3319, 9.3460, 5.9358], [8.6940, 5.6772, 7.4109, 4.2940], [8.8544, 5.7390, 2.6658, 6.2745]]) . def multi_c_loss(preds, y): soft = softmaxp1(preds) softcon = np.log(soft)*-1 idx = range(len(y)) return softcon[idx, y].mean() . multi_loss(multipredsf, y) . tensor(3.3247) . multipredsf . tensor([[8.8227, 9.1500, 3.8286, 9.5931], [3.9045, 6.0090, 2.5657, 7.9364], [9.4077, 1.3319, 9.3460, 5.9358], [8.6940, 5.6772, 7.4109, 4.2940], [8.8544, 5.7390, 2.6658, 6.2745]]) . ((multipredsf.argmax(1)==y).float()).mean() . tensor(0.) . def multi_accuracy(preds, y): to_C = preds.argmax(1) return (to_C==y).float().mean() . multi_accuracy(multipredsf, y) . tensor(0.) . . multipredsf =tensor([[8.8227, 9.1500, 3.8286, 9.5931], [3.9045, 6.0090, 2.5657, 7.9364], [9.4077, 1.3319, 9.3460, 5.9358], [8.6940, 5.6772, 7.4109, 4.2940], [8.8544, 5.7390, 2.6658, 6.2745]]) y =tensor([[1, 0, 0, 0], [1, 1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 1, 1, 1]]) . tensor([[8.8227, 9.1500, 3.8286, 9.5931], [3.9045, 6.0090, 2.5657, 7.9364], [9.4077, 1.3319, 9.3460, 5.9358], [8.6940, 5.6772, 7.4109, 4.2940], [8.8544, 5.7390, 2.6658, 6.2745]]) . y =tensor([[1, 0, 0, 0], [1, 1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 1, 1, 1]]) y . tensor([[1, 0, 0, 0], [1, 1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 1, 1, 1]]) . sig = sigmoid(multipredsf) sig . tensor([[0.9999, 0.9999, 0.9787, 0.9999], [0.9802, 0.9975, 0.9286, 0.9996], [0.9999, 0.7912, 0.9999, 0.9974], [0.9998, 0.9966, 0.9994, 0.9865], [0.9999, 0.9968, 0.9350, 0.9981]]) . 1-((sigmoid(multipredsf)-y).abs()) . tensor([[9.9985e-01, 1.0622e-04, 2.1277e-02, 6.8188e-05], [9.8025e-01, 9.9755e-01, 7.1379e-02, 3.5739e-04], [8.2135e-05, 2.0885e-01, 8.7261e-05, 2.6361e-03], [1.6761e-04, 3.4114e-03, 6.0427e-04, 9.8653e-01], [1.4269e-04, 9.9679e-01, 9.3498e-01, 9.9812e-01]]) . def multi_l_loss(preds, y): sig = sigmoid(preds) dist = (sig-y).abs() return -np.log(1-dist).mean() . def multi_l_accuracy(preds, y, th=0.5): sig_preds = sigmoid(preds) bin_conv = (sig_preds&gt;th).float() return (bin_conv==y).float().mean() . multi_l_accuracy(multipredsf, y) . tensor(0.3500) . multi_l_loss(multipredsf, y) . tensor(4.5089) . y . tensor([[1, 0, 0, 0], [1, 1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 1], [0, 1, 1, 1]]) . (sigmoid(multipredsf)-y).abs() . tensor([[1.4734e-04, 9.9989e-01, 9.7872e-01, 9.9993e-01], [1.9753e-02, 2.4505e-03, 9.2862e-01, 9.9964e-01], [9.9992e-01, 7.9115e-01, 9.9991e-01, 9.9736e-01], [9.9983e-01, 9.9659e-01, 9.9940e-01, 1.3466e-02], [9.9986e-01, 3.2076e-03, 6.5022e-02, 1.8802e-03]]) . (-np.log(1-((sigmoid(multipredsf)-y).abs()))).mean() . tensor(4.5089) . torch.where(y==1, 1-sig, sig) . tensor([[1.4734e-04, 9.9989e-01, 9.7872e-01, 9.9993e-01], [1.9753e-02, 2.4505e-03, 9.2862e-01, 9.9964e-01], [9.9992e-01, 7.9115e-01, 9.9991e-01, 9.9736e-01], [9.9983e-01, 9.9659e-01, 9.9940e-01, 1.3466e-02], [9.9986e-01, 3.2076e-03, 6.5022e-02, 1.8802e-03]]) . (sigmoid(multipredsf)-y).abs() . tensor([[1.4734e-04, 9.9989e-01, 9.7872e-01, 9.9993e-01], [1.9753e-02, 2.4505e-03, 9.2862e-01, 9.9964e-01], [9.9992e-01, 7.9115e-01, 9.9991e-01, 9.9736e-01], [9.9983e-01, 9.9659e-01, 9.9940e-01, 1.3466e-02], [9.9986e-01, 3.2076e-03, 6.5022e-02, 1.8802e-03]]) . A = tensor([[1, 2, 2, 3], [2,4,8,2]]) A . tensor([[1, 2, 2, 3], [2, 4, 8, 2]]) . 1-A . tensor([[ 0, -1, -1, -2], [-1, -3, -7, -1]]) .",
            "url": "https://ravivkerner.github.io/blogtest/2021/12/27/test.html",
            "relUrl": "/2021/12/27/test.html",
            "date": " • Dec 27, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Tank classifier",
            "content": "!pip install -Uqq fastbook import fastbook fastbook.setup_book() from fastbook import * from fastai.vision.widgets import * . |████████████████████████████████| 720 kB 4.2 MB/s |████████████████████████████████| 46 kB 4.2 MB/s |████████████████████████████████| 1.2 MB 42.6 MB/s |████████████████████████████████| 189 kB 36.3 MB/s |████████████████████████████████| 56 kB 4.6 MB/s |████████████████████████████████| 51 kB 327 kB/s Mounted at /content/gdrive . path = Path(&#39;/content/gdrive/MyDrive/tanks&#39;) . Download imges for training . tank_types = &#39;merkava mk4&#39;,&#39;M1 Abrams&#39;,&#39;water&#39; path = Path(&#39;tanks&#39;) . if not path.exists(): path.mkdir() for o in tank_types: dest = (path/o) dest.mkdir(exist_ok=True) urls = search_images_ddg(f&#39;{o} tank&#39;, max_images=150) download_images(dest, urls=urls) . fns = get_image_files(path) failed = verify_images(fns) failed.map(Path.unlink); . Preparing the data to the model . tanks = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) . dls = tanks.dataloaders(path) . dls.valid.show_batch(max_n=4, nrows=1) . Training the model and clean the data . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . Downloading: &#34;https://download.pytorch.org/models/resnet18-f37072fd.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth . epoch train_loss valid_loss error_rate time . 0 | 1.437875 | 0.810209 | 0.333333 | 00:54 | . /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; . epoch train_loss valid_loss error_rate time . 0 | 0.536968 | 0.407019 | 0.142857 | 00:24 | . 1 | 0.393917 | 0.193613 | 0.066667 | 00:25 | . 2 | 0.297317 | 0.139224 | 0.057143 | 00:24 | . 3 | 0.249617 | 0.127603 | 0.047619 | 00:24 | . /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . def plot_top_losses_fix(interp, k, largest=True, **kwargs): losses,idx = interp.top_losses(k, largest) if not isinstance(interp.inputs, tuple): interp.inputs = (interp.inputs,) if isinstance(interp.inputs[0], Tensor): inps = tuple(o[idx] for o in interp.inputs) else: inps = interp.dl.create_batch(interp.dl.before_batch([tuple(o[i] for o in interp.inputs) for i in idx])) b = inps + tuple(o[idx] for o in (interp.targs if is_listy(interp.targs) else (interp.targs,))) x,y,its = interp.dl._pre_show_batch(b, max_n=k) b_out = inps + tuple(o[idx] for o in (interp.decoded if is_listy(interp.decoded) else (interp.decoded,))) x1,y1,outs = interp.dl._pre_show_batch(b_out, max_n=k) if its is not None: #plot_top_losses(x, y, its, outs.itemgot(slice(len(inps), None)), L(self.preds).itemgot(idx), losses, **kwargs) plot_top_losses(x, y, its, outs.itemgot(slice(len(inps), None)), interp.preds[idx], losses, **kwargs) #TODO: figure out if this is needed #its None means that a batch knows how to show itself as a whole, so we pass x, x1 #else: show_results(x, x1, its, ctxs=ctxs, max_n=max_n, **kwargs) . plot_top_losses_fix(interp, 10, nrows=10) . cleaner = ImageClassifierCleaner(learn) cleaner . . for idx, cat in cleaner.change(): f = cleaner.fns[idx] os.rename(f, f.parent/(f.parent.name+f.stem+f.suffix)) shutil.move(str(f.parent/(f.parent.name+f.stem+f.suffix)), path/cat ) . POC with user interface . Using the Model for Inference . learn.export() . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . learn_inf = load_learner(path/&#39;export.pkl&#39;) . Making multi label classifier on the same data-&gt;own loss . lr = cnn_learner(dls, resnet18, loss_func=F.binary_cross_entropy_with_logits, metrics=partial(accuracy_multi, thresh=0.2) ) . lr.fine_tune(4) . Simple UI demo . . path = Path() learn_inf = load_learner(path/&#39;multi.pkl&#39;) . btn_upload = widgets.FileUpload() out_pl = widgets.Output() lbl_pred = widgets.Label() btn_run = widgets.Button(description=&#39;Classify&#39;) . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . VBox([widgets.Label(&#39;Select your tank!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . Addtional validation . learnerm.predict(&#39;/content/126.JPG&#39;) . ((#1) [&#39;water&#39;], TensorBase([False, False, False, True]), TensorBase([0.4033, 0.0402, 0.1413, 0.9869])) . multi_learner2.predict(&#39;/content/DUDU.jpg&#39;) . ((#0) [], TensorBase([-2.2279, -0.8042, 0.6245]), TensorBase([-2.2279, -0.8042, 0.6245])) . a = torch.tensor([-2.2279, -0.8042, 0.6245]) . a.sigmoid() . tensor([0.0973, 0.3091, 0.6512]) . multi_learner2.predict(&#39;/content/TetroWtank.jpg&#39;) . ((#0) [], TensorBase([-5.6009, -2.1743, 4.8451]), TensorBase([-5.6009, -2.1743, 4.8451])) . a = torch.tensor([-5.6009, -2.1743, 4.8451]) a.sigmoid() . tensor([0.0037, 0.1021, 0.9922]) . create_cnn_model? . cnn_learner?? . multi_learner2.predict?? . multi_learner.predict(&#39;/content/DUDU.jpg&#39;) . ((#2) [&#39;merkava mk4&#39;,&#39;water&#39;], TensorBase([False, True, True]), TensorBase([0.1738, 0.9022, 0.5532])) . multi_learner.predict(&#39;/content/manytanks.jpg&#39;) . multi_learner.predict(&#39;/content/merk_abrms2.jpeg&#39;) . ((#2) [&#39;M1 Abrams&#39;,&#39;merkava mk4&#39;], TensorBase([ True, True, False]), TensorBase([0.6895, 0.9527, 0.0021])) . multi_learner.predict(&#39;/content/merkav_abrams.jpg&#39;) . ((#1) [&#39;M1 Abrams&#39;], TensorBase([ True, False, False]), TensorBase([0.9377, 0.4514, 0.0021])) . multi_learner.predict(&#39;/content/mek_abs_3.jpg&#39;) . ((#1) [&#39;M1 Abrams&#39;], TensorBase([ True, False, False]), TensorBase([1.0000, 0.0072, 0.0388])) . multi_learner.predict(&#39;/content/TetroWtank.jpg&#39;) . ((#1) [&#39;water&#39;], TensorBase([False, False, True]), TensorBase([0.0694, 0.1050, 0.9140])) . a, b, c = lr.predict(&#39;/content/merk_abrms2.jpeg&#39;) c.sigmoid() . TensorBase([0.2893, 0.9868, 0.0014]) . Try Multi label model on the sdata . def parentlabel(x): return [x.parent.name] tanks = DataBlock( blocks=(ImageBlock, MultiCategoryBlock), #MultiCategoryBlock(add_na=True) get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parentlabel, item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) . dls = tanks.dataloaders(path) . /usr/local/lib/python3.7/dist-packages/torch/_tensor.py:1051: UserWarning: torch.solve is deprecated in favor of torch.linalg.solveand will be removed in a future PyTorch release. torch.linalg.solve has its arguments reversed and does not return the LU factorization. To get the LU factorization see torch.lu, which can be used with torch.lu_solve or torch.lu_unpack. X = torch.solve(B, A).solution should be replaced with X = torch.linalg.solve(A, B) (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:766.) ret = func(*args, **kwargs) . dls.valid.show_batch(nrows=1, ncols=6) . multi_learner = cnn_learner(dls, resnet18, metrics=accuracy_multi) #threshold=0.5, Sigmod=True&gt;partial(accuracy_multi, thresh=0.95 . Downloading: &#34;https://download.pytorch.org/models/resnet18-f37072fd.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth . multi_learner.fine_tune(4) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.932164 | 0.447739 | 0.820513 | 02:26 | . /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; . epoch train_loss valid_loss accuracy_multi time . 0 | 0.448215 | 0.294454 | 0.868590 | 00:24 | . 1 | 0.357718 | 0.199940 | 0.916667 | 00:25 | . 2 | 0.282826 | 0.181428 | 0.935897 | 00:24 | . 3 | 0.237339 | 0.176398 | 0.935897 | 00:24 | . /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; . Fixing the top lose . def plot_top_losses_fix(interp, k, largest=True, **kwargs): losses,idx = interp.top_losses(k, largest) if not isinstance(interp.inputs, tuple): interp.inputs = (interp.inputs,) if isinstance(interp.inputs[0], Tensor): inps = tuple(o[idx] for o in interp.inputs) else: inps = interp.dl.create_batch(interp.dl.before_batch([tuple(o[i] for o in interp.inputs) for i in idx])) b = inps + tuple(o[idx] for o in (interp.targs if is_listy(interp.targs) else (interp.targs,))) x,y,its = interp.dl._pre_show_batch(b, max_n=k) b_out = inps + tuple(o[idx] for o in (interp.decoded if is_listy(interp.decoded) else (interp.decoded,))) x1,y1,outs = interp.dl._pre_show_batch(b_out, max_n=k) if its is not None: #plot_top_losses(x, y, its, outs.itemgot(slice(len(inps), None)), L(self.preds).itemgot(idx), losses, **kwargs) plot_top_losses(x, y, its, outs.itemgot(slice(len(inps), None)), interp.preds[idx], losses, **kwargs) #TODO: figure out if this is needed #its None means that a batch knows how to show itself as a whole, so we pass x, x1 #else: show_results(x, x1, its, ctxs=ctxs, max_n=max_n, **kwargs) . multi_learner.dls.vocab . [&#39;M1 Abrams&#39;, &#39;merkava mk4&#39;, &#39;water&#39;] . intrpt = ClassificationInterpretation.from_learner(multi_learner) . /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; . plot_top_losses_fix(intrpt, 10, nrows=10) . multi_learner.metrics = partial(accuracy_multi, thresh=0.2) . multi_learner.validate() . /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; . (#2) [0.17249105870723724,0.9070513248443604] . preds, targs = multi_learner.get_preds() . preds.shape, targs.shape . (torch.Size([104, 3]), torch.Size([104, 3])) . preds[0], targs[0] . (TensorBase([0.9940, 0.0058, 0.0028]), TensorMultiCategory([1., 0., 0.])) . accuracy_multi(preds, targs, thresh=0.5, sigmoid=False) . TensorBase(0.9423) . th = [0.1, 0.3, 0.5, 0.7, 0.9] . [accuracy_multi(preds, targs, thresh=x, sigmoid=False) for x in th] . [TensorBase(0.8750), TensorBase(0.9167), TensorBase(0.9423), TensorBase(0.9487), TensorBase(0.9487)] . xs = torch.linspace(0.05,0.95,29) accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs] plt.plot(xs,accs); . multi_learner.loss_func.thresh=0.2 . multi_learner.loss_func.activation . &lt;bound method BCEWithLogitsLossFlat.activation of FlattenedLoss of BCEWithLogitsLoss()&gt; . multi_learner.loss_func . FlattenedLoss of BCEWithLogitsLoss() . multi_learner.loss_func.decodes . &lt;bound method BCEWithLogitsLossFlat.decodes of FlattenedLoss of BCEWithLogitsLoss()&gt; . multi_learner.export(&#39;multi&#39;) . same model but with assign loss function . # This is formatted as code . ls=nn.BCEWithLogitsLoss() . multi_learner2 = cnn_learner(dls, resnet18, loss_func=ls, metrics=accuracy_multi) . multi_learner2.fine_tune(4) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.872250 | 0.800857 | 0.753205 | 00:24 | . /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; . epoch train_loss valid_loss accuracy_multi time . 0 | 0.430897 | 0.460585 | 0.833333 | 00:25 | . 1 | 0.379967 | 0.258608 | 0.916667 | 00:25 | . 2 | 0.311630 | 0.176938 | 0.939103 | 00:25 | . 3 | 0.259250 | 0.157732 | 0.935897 | 00:25 | . /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; . multi_learner2.get_preds(with_decoded=False) . /usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images &#34;Palette images with Transparency expressed in bytes should be &#34; . (TensorBase([[ 1.1054e+01, -4.3865e+00, -3.8246e+00], [-4.8288e+00, -6.0278e+00, 1.2231e+01], [-4.8429e+00, 8.0363e-02, 8.5969e-01], [-1.1705e+00, -5.7417e+00, 1.0362e+01], [-4.0937e+00, -2.2996e+00, 9.4857e+00], [-3.8556e+00, -4.1440e+00, 1.2390e+01], [-3.8271e+00, 6.5946e+00, -4.1401e+00], [ 1.0715e+01, -5.2831e+00, -6.1691e+00], [-1.2443e+00, 4.7203e+00, -5.1741e+00], [-3.3200e+00, -2.3427e+00, 7.2592e+00], [-3.9163e+00, -2.3793e+00, 9.4112e+00], [-3.1000e+00, -2.0570e+00, 6.1245e+00], [-2.9966e+00, -2.6585e+00, 1.0127e+01], [ 1.2771e+01, -6.1638e+00, -5.5002e+00], [-2.3349e+00, -2.4252e+00, 8.9080e+00], [ 1.2209e+01, -8.3553e+00, -6.7067e+00], [-5.0397e+00, -2.6586e+00, 1.0207e+01], [ 1.0072e+01, -2.9396e+00, -6.1747e+00], [-4.9629e+00, -5.0143e+00, 1.2457e+01], [-2.4362e+00, -3.4457e+00, 1.1917e+01], [-3.3012e+00, -4.1240e+00, 9.7303e+00], [-2.6708e+00, 6.3316e+00, -5.4875e+00], [-4.2726e+00, -4.0000e+00, 8.1809e+00], [-4.7021e+00, -6.8715e+00, 1.3693e+01], [ 1.1156e+01, -7.3597e+00, -4.5801e+00], [-5.1651e+00, -4.5510e+00, 1.7812e+01], [-2.6461e+00, -5.3205e+00, 1.2895e+01], [ 1.0624e+01, -6.1674e+00, -7.2841e+00], [-3.9826e+00, -3.9549e+00, 9.2268e+00], [ 1.0253e+01, -4.5484e+00, -6.0170e+00], [-2.3421e+00, 5.4049e+00, -3.4479e+00], [ 3.7226e+00, -5.4512e+00, -9.5822e-01], [-3.8336e+00, 7.4639e+00, -2.5219e+00], [-4.4530e+00, -4.9878e+00, 1.5383e+01], [ 8.7228e+00, -3.2869e+00, -6.2374e+00], [-4.7827e+00, 1.1229e+01, -7.6850e+00], [-3.2945e+00, -3.4483e+00, 6.4119e+00], [ 9.8750e+00, -5.1142e+00, -8.9492e+00], [ 9.5270e+00, -9.3801e+00, -7.7053e+00], [ 1.6784e+01, -8.5052e+00, -8.4742e+00], [-5.0142e+00, -8.1198e+00, 1.0895e+01], [ 1.3255e+00, 1.6833e+00, -2.7004e+00], [ 1.7176e+01, -1.0126e+01, -9.0501e+00], [-3.8401e+00, -7.1575e+00, 1.6060e+01], [-3.3630e+00, -3.8490e+00, 1.2150e+01], [ 8.3805e-01, 3.2154e-01, -4.7388e+00], [-4.5516e+00, 8.7604e+00, -6.0815e+00], [-1.9349e+00, -5.9613e+00, 1.3269e+01], [-5.3436e+00, -4.0963e+00, 9.9250e+00], [ 1.0194e+01, -3.8770e+00, -5.9039e+00], [ 1.1517e+01, -6.8700e+00, -8.5469e+00], [-3.3945e+00, 6.4746e+00, -4.0815e+00], [ 6.8645e+00, -2.5304e+00, -4.9125e+00], [-4.3031e+00, 7.2538e+00, -3.6068e+00], [ 7.9635e+00, -6.2923e+00, -4.8670e+00], [-2.2701e+00, -5.0116e+00, 8.7348e+00], [ 1.2780e+01, -3.8046e+00, -7.6140e+00], [ 8.5537e-03, 1.8284e+00, -5.9196e+00], [-3.5084e-01, 1.1758e+00, -4.3755e+00], [ 7.3451e+00, -4.7630e+00, -5.5742e+00], [-4.3683e+00, -6.3360e+00, 1.4351e+01], [-6.8580e+00, -4.3077e-01, 6.4575e+00], [ 1.5667e+01, -4.7154e+00, -8.2658e+00], [ 9.2235e+00, -5.7826e+00, -4.5370e+00], [-7.4300e-01, 4.6694e+00, -6.4751e+00], [-4.1144e+00, -3.3529e+00, 8.9280e+00], [ 1.3519e+01, -5.3617e+00, -6.0206e+00], [ 1.1722e+01, -6.0788e+00, -7.7755e+00], [-5.9252e+00, 1.0020e+01, -6.6066e+00], [-5.0563e+00, -7.5295e+00, 1.5377e+01], [ 3.6416e-01, 7.0579e+00, -8.8655e+00], [-4.4168e+00, -5.0467e+00, 8.1200e+00], [ 1.2095e+01, -4.9284e+00, -5.3975e+00], [-3.5256e+00, -2.0509e+00, 7.1033e+00], [ 3.7408e+00, -1.7004e-01, -7.8555e+00], [ 2.8994e+00, 1.7881e+00, -4.6694e+00], [ 8.7549e+00, -5.0893e+00, -6.9562e+00], [ 1.8290e+01, -6.4178e+00, -9.8740e+00], [ 1.1128e+01, -4.8088e+00, -9.0944e+00], [ 6.3693e+00, -3.6620e+00, -6.3378e+00], [-2.6944e+00, 4.9052e+00, -6.6235e+00], [-5.1428e+00, -5.0365e+00, 1.1248e+01], [ 1.4320e+00, 7.9101e-01, -5.2702e+00], [-1.6046e+00, 4.2638e+00, -4.9068e+00], [-4.9069e+00, 7.9584e+00, -5.2792e+00], [-3.2035e+00, -1.7164e+00, 1.0915e+01], [ 4.9181e+00, -2.4524e+00, -3.6611e+00], [-5.2584e+00, -4.5208e+00, 1.1430e+01], [-7.7838e-01, 4.1009e+00, -5.6056e+00], [ 1.1177e+01, -5.5141e+00, -5.9799e+00], [-3.1584e+00, -4.7642e+00, 9.7285e+00], [-4.4772e+00, -2.2261e+00, 9.7936e+00], [ 2.0223e+00, 2.5525e+00, -6.6029e+00], [-4.8925e+00, -3.6348e+00, 1.1941e+01], [ 2.8736e+00, 8.6336e-01, -5.2133e+00], [-3.2290e+00, 5.8168e+00, -4.3457e+00], [ 8.9244e-01, 5.9619e+00, -8.6835e+00], [ 2.8697e-02, 3.5361e+00, -5.8552e+00], [ 4.0667e+00, 2.7531e+00, -6.2681e+00], [-3.0422e+00, -4.2448e+00, 1.2542e+01], [-4.6983e+00, -1.7849e+00, 1.2067e+01], [ 4.2613e+00, 5.1500e-01, -7.1514e+00], [-3.4810e+00, -6.4663e+00, 1.6082e+01], [ 2.6145e-01, -3.9896e+00, 6.7493e+00]]), TensorMultiCategory([[1., 0., 0.], [0., 0., 1.], [0., 1., 0.], [0., 0., 1.], [0., 0., 1.], [0., 0., 1.], [0., 1., 0.], [1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 0., 1.], [0., 0., 1.], [0., 0., 1.], [1., 0., 0.], [0., 0., 1.], [1., 0., 0.], [0., 0., 1.], [1., 0., 0.], [0., 0., 1.], [0., 0., 1.], [0., 0., 1.], [0., 1., 0.], [0., 0., 1.], [0., 0., 1.], [1., 0., 0.], [0., 0., 1.], [0., 0., 1.], [1., 0., 0.], [0., 0., 1.], [1., 0., 0.], [0., 1., 0.], [1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [1., 0., 0.], [1., 0., 0.], [1., 0., 0.], [0., 0., 1.], [0., 1., 0.], [1., 0., 0.], [0., 0., 1.], [0., 0., 1.], [1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 0., 1.], [1., 0., 0.], [1., 0., 0.], [0., 1., 0.], [1., 0., 0.], [0., 1., 0.], [1., 0., 0.], [0., 0., 1.], [1., 0., 0.], [1., 0., 0.], [1., 0., 0.], [1., 0., 0.], [0., 0., 1.], [0., 0., 1.], [1., 0., 0.], [1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [1., 0., 0.], [1., 0., 0.], [0., 1., 0.], [0., 0., 1.], [0., 1., 0.], [0., 0., 1.], [1., 0., 0.], [0., 0., 1.], [1., 0., 0.], [0., 1., 0.], [1., 0., 0.], [1., 0., 0.], [1., 0., 0.], [0., 1., 0.], [0., 1., 0.], [0., 0., 1.], [1., 0., 0.], [0., 1., 0.], [0., 1., 0.], [0., 0., 1.], [1., 0., 0.], [0., 0., 1.], [1., 0., 0.], [1., 0., 0.], [0., 0., 1.], [0., 0., 1.], [0., 1., 0.], [0., 0., 1.], [0., 1., 0.], [0., 1., 0.], [0., 1., 0.], [0., 1., 0.], [1., 0., 0.], [0., 0., 1.], [0., 0., 1.], [1., 0., 0.], [0., 0., 1.], [0., 0., 1.]])) .",
            "url": "https://ravivkerner.github.io/blogtest/2021/12/27/loss.html",
            "relUrl": "/2021/12/27/loss.html",
            "date": " • Dec 27, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ravivkerner.github.io/blogtest/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ravivkerner.github.io/blogtest/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ravivkerner.github.io/blogtest/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ravivkerner.github.io/blogtest/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}